{"cells":[{"cell_type":"code","source":["#extract raw data for all SQL tables in ADF or other orchestration...extract to data lake raw\n#get snapshot raw data and merge to delta...upsert changes using this code"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3266c83f-9ecb-485a-815f-e77ee2404953"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def extract_process_view_table(subject:str):\n  \"\"\"\n  extract view tables and write back to a delta table\n  currently the fucntion overwtites existing data. (Full Load)\n  \"\"\"\n  import os\n  from datetime import datetime \n  from pyspark.sql.functions import col , to_timestamp , lit , row_number\n  from pyspark.sql.window import Window\n  \n  path = '/dbfs/mnt/datalake_raw/batch/view_us/{object}'.format(object=subject)\n  save_to = \"/mnt/datalake_premium/view_us/{object}\".format(object=subject)\n  # get most recent files - (today's files)\n  fdpaths = [path+\"/\"+fd for fd in os.listdir(path)]\n  files_to_process = []\n  for fdpath in fdpaths:\n      statinfo = os.stat(fdpath)\n      modified_date = datetime.fromtimestamp(statinfo.st_mtime)\n      if modified_date.date() == datetime.today().date():\n         files_to_process.append(fdpath.replace('/dbfs',''))\n  \n  # process the data \n  if files_to_process:\n    df = (spark.read.format('json')\n    .option('inferSchema','true')\n    .option(\"timestampFormat\",\"yyyy-MM-dd'T'hh:mm:SS\")\n    .load(files_to_process))\n\n    # remove spaces from column names \n    df = df.select([col(x).alias(x.replace(' ','')) for x in df.columns])\n    # check if lastUpdated exists \n    if 'LastUpdated' in df.columns:\n      df = df.withColumn(\"LastUpdated\",to_timestamp(col(\"LastUpdated\")))\n\n    df = df.drop('ELTLoadDateTime','LoadDateTime').dropDuplicates()\n    # write to delta table \n    (df\n    .coalesce(1)\n    .write\n    .format('delta')\n    .mode(\"overwrite\")\n    .option(\"overwriteSchema\", \"true\")\n    #.option(\"mergeSchema\", \"true\")\n    .save(save_to)\n    )"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"98df2d11-ac88-4aff-b7a5-b1ea982f9103"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["import os\n# What are futures?\nimport concurrent.futures\npath = '/dbfs/mnt/datalake_raw/batch/view_us/'\nsubjects_to_process =  [fd for fd in os.listdir(path)]\n# for subject in subjects_to_process:\n#   extract_process_view_table(subject)\n\nwith concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:\n  future_to_tran = {executor.submit(extract_process_view_table, subject): subject for subject in subjects_to_process}\n  for future in concurrent.futures.as_completed(future_to_tran):\n      tran = future_to_tran[future]\n      try:\n          result = future.result()\n      except Exception as exc:\n          print('%r generated an exception: %s' % (tran, exc))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d5c58cf5-3dec-4ce5-9b77-0299c7ca5277"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">&#39;service_cr_digital_option_units&#39; generated an exception: Data used in creating the Delta table doesn&#39;t have any columns.\n&#39;service_cr_digital_options&#39; generated an exception: Data used in creating the Delta table doesn&#39;t have any columns.\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">&#39;service_cr_digital_option_units&#39; generated an exception: Data used in creating the Delta table doesn&#39;t have any columns.\n&#39;service_cr_digital_options&#39; generated an exception: Data used in creating the Delta table doesn&#39;t have any columns.\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["for tbl in subjects_to_process:\n  sql_s = \"CREATE TABLE if not exists ods.view_us_{tbl} USING DELTA LOCATION '/mnt/datalake_premium/view_us/{tbl}'\".format(tbl=tbl)\n  \n  try:\n    spark.sql(sql_s)\n  except Exception as exc:\n    print('%r generated an exception: %s' % (sql_s, exc))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e517b7ef-d0ba-432c-9701-af72a67b2966"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">&#34;CREATE TABLE if not exists ods.view_us_service_cr_digital_option_units USING DELTA LOCATION &#39;/mnt/datalake_premium/view_us/service_cr_digital_option_units&#39;&#34; generated an exception: \nYou are trying to create an external table `ods`.`view_us_service_cr_digital_option_units`\nfrom `/mnt/datalake_premium/view_us/service_cr_digital_option_units` using Databricks Delta, but the schema is not specified when the\ninput path is empty.\n\nTo learn more about Delta, see https://docs.microsoft.com/azure/databricks/delta/index\n       \n&#34;CREATE TABLE if not exists ods.view_us_service_cr_digital_options USING DELTA LOCATION &#39;/mnt/datalake_premium/view_us/service_cr_digital_options&#39;&#34; generated an exception: \nYou are trying to create an external table `ods`.`view_us_service_cr_digital_options`\nfrom `/mnt/datalake_premium/view_us/service_cr_digital_options` using Databricks Delta, but the schema is not specified when the\ninput path is empty.\n\nTo learn more about Delta, see https://docs.microsoft.com/azure/databricks/delta/index\n       \n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">&#34;CREATE TABLE if not exists ods.view_us_service_cr_digital_option_units USING DELTA LOCATION &#39;/mnt/datalake_premium/view_us/service_cr_digital_option_units&#39;&#34; generated an exception: \nYou are trying to create an external table `ods`.`view_us_service_cr_digital_option_units`\nfrom `/mnt/datalake_premium/view_us/service_cr_digital_option_units` using Databricks Delta, but the schema is not specified when the\ninput path is empty.\n\nTo learn more about Delta, see https://docs.microsoft.com/azure/databricks/delta/index\n       \n&#34;CREATE TABLE if not exists ods.view_us_service_cr_digital_options USING DELTA LOCATION &#39;/mnt/datalake_premium/view_us/service_cr_digital_options&#39;&#34; generated an exception: \nYou are trying to create an external table `ods`.`view_us_service_cr_digital_options`\nfrom `/mnt/datalake_premium/view_us/service_cr_digital_options` using Databricks Delta, but the schema is not specified when the\ninput path is empty.\n\nTo learn more about Delta, see https://docs.microsoft.com/azure/databricks/delta/index\n       \n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["dbutils.notebook.exit(\"Job Completed Successfuly!\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d8f1c99e-27c2-4906-8d73-4bd5270c16d7"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"exit","data":"Job Completed Successfuly!","arguments":{},"metadata":{}}},"output_type":"display_data","data":{"text/plain":["Job Completed Successfuly!"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"View - Full Load (1)","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2,"widgetLayout":[]},"language":"python","widgets":{"subjects_to_process":{"nuid":"9888c761-8e14-4d53-8f79-d30cb3169652","currentValue":"","widgetInfo":{"widgetType":"text","name":"subjects_to_process","defaultValue":"","label":null,"options":{"widgetType":"text","validationRegex":null}}}},"notebookOrigID":3307907406259334}},"nbformat":4,"nbformat_minor":0}
